---
title: "Key Findings"
output: html_document
---

```{r include=FALSE}
#Customize ggplot theme
theme_set(theme_minimal())
theme_update(axis.text.x = element_text(angle = 60, hjust = 1))
theme_update(plot.title = element_text(color="#666666", face="bold", size=22, hjust=0))
theme_update(axis.title = element_text(color="#666666", face="bold", size=18))
theme_update(plot.title = element_text(hjust = 0.5))
```

# Key Findings

## Machine Learning Predictions

> We set out on this project to answer one question, "Can we use machine learning to predict which Kickstarter projects will get funded?" As it turns out, we can! 

> __Using relatively simple machine learning techniques and feature engineering, we raised our baseline accuracy from 56% to 
70% (a 25% improvement)!__

> First, it is essential to establish a baseline metric for success. Given that in our collected data projects were funded at a rate of 56%, a model in the absolute simplest version would predict that every project was funded and be correct 56% of the time. For our predictions to have any value, we must beat this benchmark. During our exploratory analysis, we discovered a variable with exciting implications, `staff pick`. As machine learning applications become more widely used and increase in efficacy, often the benchmark is, "Can it do better than a human?" `staff_pick` serves as a proxy for the best human judgment has to offer with 84% receiving funding. [Kickstarter - Projects We Love](https://www.kickstarter.com/discover/recommended)

```{r echo = FALSE}
df_engr %>%
  group_by(staff_pick) %>%
    summarise(count = n(), avg_funded = mean(funded)) %>% 
  ungroup(staff_pick)
```

> Kickstarter staff clearly has an eye for promising projects. However, predicting project success anywhere near this rate is likely impossible. Projects fortunate enough to be a `staff_pick` get prominently featured on the website, newsletters, and blogs. We do not doubt that such promotion materially effects the chance a project is funded. 

> Scoping our target prediction accuracy range to 56-84%, we began construction models.  We built a LASSO Regression and Decision Tree models. Incorporating variables about the campaign such as `goal`, `rewards`, `category`, and `usa` along with information from the creator's profile such as `social_media_count` both models achieved an accuracy rate of ~70%--right in the middle of our target range. 


***
## `percent_funded`
> This histogram of `percent_funded` was one of the most interesting we saw during the project. Notice that almost every project that achieves ~75% of their goal made it to, or well past 100% (we found an outlier at 4 million percent). __Virtually no projects fall just short of their goal!__

> This generates significant insights into the workings of Kickstarter's business model. Each project has two key financial stakeholders, the project creator and Kickstarter itself (who collects a 5% fee on funded projects), each willing to pull whatever levers it can to avoid the worst case scenario--a 99% funded project. You can imagine, an almost funded creator will call all his family and friends--or even open their own wallet--to get a project across the finish line. Kickstarter will unleash all its marketing power through its almost funded page, newsletters, and emails to make the goal. 

> This insight should have a direct impact on a creator's goal-setting strategy. While we have seen that higher goals have lower funding rates, we actually encourage creators to set aggressive goals for reasonable absolute sums. For example, if a creator believes their project needs $1,000 in funding and can most likely procure the necessary backers, he should strongly consider raising the goal to $1,200-$1,300. In so doing, he would allow the regular backer support to raise the first $1,000 and then let his business partner (Kickstarter) aggressively market the project for him to raise the addition $200+, thereby covering the 5% fee and more.

```{r echo=FALSE, message=FALSE, warning=FALSE}

df_filtered_by_percent_funded <- df_engr %>%
  filter(!is.na(percent_funded) & percent_funded <= 400)

# Non-normal distribution
df_filtered_by_percent_funded %>% 
  ggplot(aes(x = percent_funded)) + 
  geom_histogram(fill = "#332288") +
  labs(title = "Histogram of Percent Funded", x = "Pledged/Goal (%)", y = "Number of Projects")

#Clean up
rm(df_filtered_by_percent_funded)
```

> Initially, we thought percent funded would be a good continuous outcome variable for predictive modeling. It is more precise than binary classification, which required selecting a somewhat arbitrary decision threshold. However, plotting the histogram shows that the data has a non-normal distribution. Because of this pattern, `percent_funded` was unsuitable for predictive modeling. This distribution also prevented conducting any natural experiments, such as regression discontinuity, to determine differences between barely-failed and barely-successful projects. 

***
## In-progress Causality
> During our exploratory analysis, we discovered many interesting variables. However, it quickly became apparent that our project as a complicated confounding variable, the point in time the project was observed. Our data is collected from the project as it exists now--completed.

> The significance of this is that as we build our models to predict the outcome of a project at its launch, we have access to information from the "future," beyond just the outcome variable of `funded`. Examples include `comments_count`, `updates count`, and whether a project gets featured on the almost funded page. We will use `comments_count` to dive a bit deeper.

```{r echo = FALSE}
df_commments = data.frame(funded = df_engr$funded, comments_count = db_cleaned$comments_count)
df_commments$df_cc_20 = cut(df_commments$comments_count, breaks = unique(quantile(df_commments$comments_count, seq(0, 1, by = .05))), include.lowest = TRUE)

df_commments %>% 
  group_by(df_cc_20) %>%
  summarise(avg_funded = mean(funded)) %>%
  ggplot(aes(x = df_cc_20, y = avg_funded)) +
  geom_bar(stat="identity", fill = "#332288") +
  labs(title = "Funding By Comments",
       x="Number of Comments",
       y="Chance of Funding")

rm(df_commments)
```

> We can see an obvious correlation, as `comments_count` increases so does its chance of funding. Unsurprisingly (this is data analytics after all) we find ourselves wrestling with questions of causation. "Do comments cause a project to get funded?" "Or, do great projects that are destined for funding prone to receiving more comments?" 

> Answering this question would require tracking projects over time while they are active and performing controlled experiments. For example, we could build two virtually identical projects and begin seeding the comments of only one, allowing us to identify the causal impact of seeding a project's comment section.

> Luckily, we don't have to validate every hypothesis with statistical rigor to begin making business decisions. Our intuition tells us that the impact of `comments_count` is surely a mixture of causality. We can, therefore, recommend to creators that in addition to designing a compelling project, find a backer or two who are passionate about your project and encourage them to start a discussion on the project page. It might be just what you need to get tip the scales in your favor.
