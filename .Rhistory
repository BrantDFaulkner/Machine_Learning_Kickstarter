m <- lm(Total ~ Index, data = d)
summary(m)
y1 <- predict(m)
fitted <- m$fitted.values
plot(fitted)
plot(d$Index, d$Total - y1,
main = "Detrended Total Daily Bike Demand",
col = 4,
xlab = 'Day',
ylab = 'Detrended Demand')
scatterplotMatrix( ~ Total + meanatemp + meanhumidity + meanwindspeed,
data = d)
library(cars)
library(car)
scatterplotMatrix( ~ Total + meanatemp + meanhumidity + meanwindspeed,
data = d)
# Boxplot by season
boxplot(d$Total ~ d$season,
main = 'Boxplot of Total Bike Demand By Season',
xlab = 'Season',
ylab = 'Demand',
col = c(5, 6, 7, 8))
# train and test split
n <- dim(d)
print(n)
ind <- sample.int(n, size = n*0.2)
dtrain <- d[-ind,]
dtest <- d[ind,]
print(dim(dtrain))
print(dim(dtest))
# Construct linear regression of all explanatory variables
# Store the regression formula as a variable to avoid re-typing all variables every time
F1 <- as.formula(
"Total ~
Index + month + season + holiday +
meanatemp + maxatemp + minatemp + sdatemp +
meanhumidity + maxhumidity + minhumidity + sdhumidity +
meanwindspeed + maxwindspeed + minwindspeed + sdwindspeed")
F1
class(F1)
m <- lm(F1, data = dtrain)
summary(m)
# Predict outcome of test sample
p1 <- predict(m, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p1 <- dtest$Total - p1
RMSPE_p1 <- sqrt(mean(Error_p1^2))
RMSPE_p1
# Install required packages
library(car)
library(MASS)
library(glmnet)
# Format outcome variable as a vector and explanatory variables as a matrix for LASSO
y <- dtrain$Total
x <- model.matrix(m)
View(x) # needs to have a leading column of 1's followed by all the variables
# Find the optimal lambda
# This function does a bunch of cross-validation to find the best penalization parameter
n <- cv.glmnet(x, y, family = "gaussian")
n$lambda.min
plot(n)
n$cvm
# Use the optimal lambda to build the LASSO model
lasso <- glmnet(x, y, family = "gaussian", lambda = n$lambda.min)
lasso$beta
# Predict outcome of test sample
p2 <- predict(lasso, newdata = dtest)
# Install required packages
library(car)
library(MASS)
library(glmnet)
# Format outcome variable as a vector and explanatory variables as a matrix for LASSO
y <- dtrain$Total
x <- model.matrix(m)
# Find the optimal lambda
# This function does a bunch of cross-validation to find the best penalization parameter
n <- cv.glmnet(x, y, family = "gaussian")
n$lambda.min
plot(n)
n$cvm
# Use the optimal lambda to build the LASSO model
lasso <- glmnet(x, y, family = "gaussian", lambda = n$lambda.min)
lasso$beta
# Predict outcome of test sample
p2 <- predict(lasso, newdata = dtest)
?predict.glmnet
# Install required packages
library(car)
library(MASS)
library(glmnet)
# Format outcome variable as a vector and explanatory variables as a matrix for LASSO
y <- dtrain$Total
x <- model.matrix(m)
# Find the optimal lambda
# This function does a bunch of cross-validation to find the best penalization parameter
n <- cv.glmnet(x, y, family = "gaussian")
n$lambda.min
plot(n)
n$cvm
# Use the optimal lambda to build the LASSO model
lasso <- glmnet(x, y, family = "gaussian", lambda = n$lambda.min)
lasso$beta
# Create matrix of test dataset
m2 <- lm(F1, data = dtest)
summary(m2)
xtest <- model.matrix(m2)
# Predict outcome of test sample
p2 <- predict(lasso, newx = xtest, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p2 <- dtest$Total - p2
RMSPE_p2 <- sqrt(mean(Error_p2^2))
RMSPE_p2
# Load required package
library(randomForest)
# Build the model
# anything more than 500 random trees would be too computationally expensive
p3 <- randomForest(F1, data = dtrain, ntree = 500)
# Use the model to predict the test sample
predict(p3, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p3 <- dtest$Total - p3
?predict.randomForest
p3
l <- predict(p3, newdata = dtest)
l
as.data.frame(l)
Error_p3 <- dtest$Total - as.data.frame(l)
RMSPE_p3 <- sqrt(mean(Error_p3^2))
RMSPE_p3
# Load required package
library(e1071)
# Build the model
p4 <- svm(F1, data = dtrain, kernel = "radial")
# Use the model to predict the test sample
predict(p4, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p4 <- dtest$Total - p4
# Load required package
library(randomForest)
# Build the model
# anything more than 500 random trees would be too computationally expensive
rf <- randomForest(F1, data = dtrain, ntree = 500)
# Use the model to predict the test sample
p3 <- predict(p3, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p3 <- dtest$Total - as.data.frame(p3)
RMSPE_p3 <- sqrt(mean(Error_p3^2))
RMSPE_p3
# Load required package
library(randomForest)
# Build the model
# anything more than 500 random trees would be too computationally expensive
rf <- randomForest(F1, data = dtrain, ntree = 500)
# Use the model to predict the test sample
p3 <- predict(rf, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p3 <- dtest$Total - as.data.frame(p3)
RMSPE_p3 <- sqrt(mean(Error_p3^2))
RMSPE_p3
# Load required package
library(e1071)
# Build the model
svm <- svm(F1, data = dtrain, kernel = "radial")
# Use the model to predict the test sample
p4 <- predict(svm, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p4 <- dtest$Total - as.data.frame(p4)
RMSPE_p4 <- sqrt(mean(Error_p4^2))
RMSPE_p4
plot(p3)
plot(p3, dtest$Total)
plot(p3)
plot(dtest$Total)
# Compare results of regression forecast to LASSO model
cbind(lasso, coef(m))
?cbind
# Compare results of regression forecast to LASSO model
cbind(coef(m), lasso)
# Install required packages
library(car)
library(MASS)
library(glmnet)
# Format outcome variable as a vector and explanatory variables as a matrix for LASSO
y <- dtrain$Total
x <- model.matrix(F1)
# Install required packages
library(car)
library(MASS)
library(glmnet)
# Format outcome variable as a vector and explanatory variables as a matrix for LASSO
y <- dtrain$Total
x <- model.matrix(F1, data = dtrain)
# Find the optimal lambda
# This function does a bunch of cross-validation to find the best penalization parameter
n <- cv.glmnet(x, y, family = "gaussian")
n$lambda.min
plot(n)
n$cvm
# Use the optimal lambda to build the LASSO model
lasso <- glmnet(x, y, family = "gaussian", lambda = n$lambda.min)
lasso$beta
# Create matrix of test dataset
m2 <- lm(F1, data = dtest)
summary(m2)
xtest <- model.matrix(m2)
# Predict outcome of test sample
p2 <- predict(lasso, newx = xtest, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p2 <- dtest$Total - p2
RMSPE_p2 <- sqrt(mean(Error_p2^2))
RMSPE_p2
# Install required packages
library(car)
library(MASS)
library(glmnet)
# Format outcome variable as a vector and explanatory variables as a matrix for LASSO
y <- dtrain$Total
x <- model.matrix(F1, data = dtrain)
# Find the optimal lambda
# This function does a bunch of cross-validation to find the best penalization parameter
n <- cv.glmnet(x, y, family = "gaussian")
n$lambda.min
plot(n)
n$cvm
# Use the optimal lambda to build the LASSO model
lasso <- glmnet(x, y, family = "gaussian", lambda = n$lambda.min)
lasso$beta
# Create matrix of test dataset
xtest <- model.matrix(F1, data = dtest)
# Predict outcome of test sample
p2 <- predict(lasso, newx = xtest, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p2 <- dtest$Total - p2
RMSPE_p2 <- sqrt(mean(Error_p2^2))
RMSPE_p2
# Compare results of regression forecast to LASSO model
cbind(coef(m), lasso)
# Compare results of regression forecast to LASSO model
cbind(coef(m), lasso$beta)
library(ggplot)
library(ggplot2)
?gettree
getTree
?getTree
getTree(p3, k = 1, labelVar = TRUE)
getTree(p3, 1, labelVar = TRUE)
getTree(rf, k = 1, labelVar = TRUE)
max(d$maxatemp)
plot.svm(svm, dtest)
# Load required package
library(e1071)
# Build the model
svm <- svm(F1, data = dtrain, kernel = "radial")
# Use the model to predict the test sample
p4 <- predict(svm, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p4 <- dtest$Total - as.data.frame(p4)
RMSPE_p4 <- sqrt(mean(Error_p4^2))
RMSPE_p4
# Visualize SVM model inputs, classes, and support vectors
plot.svm(svm, dtest)
?plot.svm
plot(svm, dtest)
plot(svm, dtrain)
library(ggplot2)
ggplot() +
geom_point(aes(x = Index, y = Total, data = d)) +
geom_line(aes(x = x, y = m$fitted.values))
ggplot() +
geom_point(aes(x = Index, y = Total, data = d)) +
geom_line(aes(x = x, y = m$fitted.values, data = m))
ggplot() +
geom_point(aes(x = Index, y = Total), data = d) +
geom_line(aes(x = x, y = m$fitted.values), data = m)
ggplot() +
geom_point(aes(x = Index, y = Total), data = d) +
geom_line(aes(x = x, y = m$fitted.values))
# Visualize results of regression forecast
ggplot() +
geom_point(aes(x = Index, y = Total), data = d) +
geom_line(aes(x = Index, y = m$fitted.values))
ggplot() +
geom_point(aes(x = Index, y = Total), data = dtrain) +
geom_line(aes(x = Index, y = m$fitted.values))
dtrain %>%
ggplot() +
geom_point(aes(x = Index, y = Total)) +
geom_line(aes(x = Index, y = m$fitted.values))
ggplot() +
geom_point(aes(x = Index, y = Total), data = dtrain) +
geom_line(aes(x = dtrain$Index, y = m$fitted.values))
RMSPE_p4 <- sqrt(mean(Error_p4^2))
plot(svm, dtrain, Total ~ Index)
plot(svm, dtrain, Total ~ Index)
v <- plot(svm, dtrain, Total ~ Index)
View(v)
?plot
plot(svm, dtrain, Total ~ Index)
dev.off()
plot(svm, dtrain, Total ~ Index)
knitr::opts_chunk$set(echo = TRUE)
plot(svm, dtrain, Total ~ Index)
plot(svm, dtrain)
# Load required package
library(e1071)
# Build the model
svm <- svm(F1, data = dtrain, kernel = "radial")
# Use the model to predict the test sample
p4 <- predict(svm, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p4 <- dtest$Total - as.data.frame(p4)
RMSPE_p4 <- sqrt(mean(Error_p4^2))
RMSPE_p4
plot(svm, dtrain)
dev.off()
plot(rnorm(50), rnorm(50))
plot(svm, dtrain)
# Construct linear regression of all explanatory variables
# Store the regression formula as a variable to avoid re-typing all variables every time
F1 <- as.formula(
"Total ~
month + season + holiday +
meanatemp + maxatemp + minatemp + sdatemp +
meanhumidity + maxhumidity + minhumidity + sdhumidity +
meanwindspeed + maxwindspeed + minwindspeed + sdwindspeed")
F1
class(F1)
m <- lm(F1, data = dtrain)
summary(m)
# Predict outcome of test sample
p1 <- predict(m, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p1 <- dtest$Total - p1
RMSPE_p1 <- sqrt(mean(Error_p1^2))
RMSPE_p1
# Construct linear regression of all explanatory variables
# Store the regression formula as a variable to avoid re-typing all variables every time
F1 <- as.formula(
"Total ~
Index + month + season + holiday +
meanatemp + maxatemp + minatemp + sdatemp +
meanhumidity + maxhumidity + minhumidity + sdhumidity +
meanwindspeed + maxwindspeed + minwindspeed + sdwindspeed")
F1
class(F1)
m <- lm(F1, data = dtrain)
summary(m)
# Predict outcome of test sample
p1 <- predict(m, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p1 <- dtest$Total - p1
RMSPE_p1 <- sqrt(mean(Error_p1^2))
RMSPE_p1
# Load required package
library(randomForest)
# Build the model
# anything more than 500 random trees would be too computationally expensive
rf <- randomForest(F1, data = dtrain, ntree = 500)
# Use the model to predict the test sample
p3 <- predict(rf, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p3 <- dtest$Total - as.data.frame(p3)
RMSPE_p3 <- sqrt(mean(Error_p3^2))
RMSPE_p3
getTree(rf, k = 1, labelVar = TRUE)
# Load required package
library(randomForest)
# Build the model
# anything more than 500 random trees would be too computationally expensive
rf <- randomForest(F1, data = dtrain, ntree = 500)
# Use the model to predict the test sample
p3 <- predict(rf, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p3 <- dtest$Total - as.data.frame(p3)
RMSPE_p3 <- sqrt(mean(Error_p3^2))
RMSPE_p3
getTree(rf, k = 1, labelVar = TRUE)
# Load required package
library(e1071)
# Build the model
svm <- svm(F1, data = dtrain, kernel = "radial")
# Use the model to predict the test sample
p4 <- predict(svm, newdata = dtest)
# Compare predicted and actual outcome in test sample
Error_p4 <- dtest$Total - as.data.frame(p4)
RMSPE_p4 <- sqrt(mean(Error_p4^2))
RMSPE_p4
?randomForest
plot(svm, dtrain)
plot(svm)
plot(svm, d)
library(readr)
testset <- read_csv("~/Downloads/testset.csv")
View(testset)
library(readr)
trainset <- read_csv("~/Downloads/trainset.csv")
View(trainset)
table(testset$Buy)
table(trainset$Buy)
table(testset$Buy)
plot(trainset$Buy, trainset$Buy, main="Scatterplot Example", xlab="Buy", ylab="Buy")
library(rpart)
library(rpart.plot)
tree <- rpart(Buy~.,trainset,method="class")
plot(tree)
text(tree)
tree
rpart.plot(tree, type=4, cex=.6)
rpart.plot(tree, type=2, cex=.6)
rpart.plot(tree, type=4, cex=.6)
predict_tree <- predict(tree, testset, type="class")
predict_tree
table(predict_tree)
table(predict_tree, testset$Buy)
?rpart
summary(tree)
plot(tree)
plot(trainset$Buy, trainset$Buy, main="Scatterplot Example", xlab="Buy", ylab="Buy")
library(rpart)
library(rpart.plot)
tree <- rpart(Buy ~ ., data = trainset, method = "class")
summary(tree)
plot(tree)
text(tree)
tree
table(trainset$Buy)
table(testset$Buy)
plot(trainset$Buy, trainset$Buy, main="Scatterplot Example", xlab="Buy", ylab="Buy")
library(rpart)
library(rpart.plot)
tree <- rpart(Buy ~ ., data = trainset, method = "class")
plot(tree)
text(tree)
tree
rpart.plot(tree, type=4, cex=.6)
rpart.plot(tree, type=2, cex=.6)
rpart.plot(tree, type=4, cex=.6)
predict_tree <- predict(tree, testset, type="class")
predict_tree
table(predict_tree)
table(predict_tree, testset$Buy)
table(trainset$Buy)
table(testset$Buy)
plot(trainset$Buy, trainset$Buy, main="Scatterplot Example", xlab="Buy", ylab="Buy")
library(rpart)
library(rpart.plot)
tree <- rpart(Buy ~ Price + Baths + Bed + SQFT + Year, data = trainset, method = "class")
plot(tree)
text(tree)
tree
rpart.plot(tree, type=4, cex=.6)
rpart.plot(tree, type=2, cex=.6)
rpart.plot(tree, type=4, cex=.6)
predict_tree <- predict(tree, testset, type="class")
predict_tree
table(predict_tree)
table(predict_tree, testset$Buy)
library(readr)
d <- read_csv("~/Documents/Academic/UIUC MBA/Spring 2018/SCA/Data/03.03_NewsvendorData.csv")
View(d)
knitr::opts_chunk$set(echo = TRUE)
hist(d$Demand)
hist(d$Demand)
hist(d$Demand)
rm(Descriptive, dtest, dtrain, Error_p2, Error_p3, Error_p4, p2, testset, trainset)
rm(casual, desc_casual, desc_reg, desc_tot, Error, Error_p1)
rm(F1, desc_cas, x, xtest, fitted, ind, Index, l)
rm(lasso, m, m1, m2, n, N, Nt)
rm(p, p1, p3, p4, predict_tree, registered, rf, RMSPE, RMSPE_p1, RMSPE_p2, RMSPE_p3, RMSPE_p4)
rm(Rows, svm, total, tree, v, y, y1)
install.packages(ks)
install.packages("ks")
k <- kde(d$Demand)
library(rgl)
k <- kde(d$Demand)
install.packages("rgl")
library(rgl)
k <- kde(d$Demand)
# Generate a uniform sample of random numbers
N = 100
u <- runif(1000)
k <- kde(d$Demand)
k <- kde(d$Demand)
library(DBI)
library(DBI)
?dbConnect
?unlink
setwd("~/Documents/Academic/UIUC MBA/Spring 2018/Independent Study/Machine_Learning_Kickstarter")
setwd("~/Documents/Academic/UIUC MBA/Spring 2018/Independent Study/Machine_Learning_Kickstarter/Data")
knitr::opts_chunk$set(echo = TRUE)
mydb <- dbConnect(RSQLite::SQLite(),"mlks_sample.db")
install.packages("RSQLite")
setwd("~/Documents/Academic/UIUC MBA/Spring 2018/Independent Study/Machine_Learning_Kickstarter/Data")
mydb <- dbConnect(RSQLite::SQLite(),"mlks_sample.db")
?dbSendQuery
db <- dbGetQuery(mydb, 'SELECT * from mlks_sample)
db <- dbGetQuery(mydb, 'SELECT * from mlks_sample')
db <- dbGetQuery(mydb, 'SELECT *')
db <- dbGetQuery(mydb, 'SELECT * FROM Projects')
db <- dbGetQuery(mydb, 'SELECT * FROM projects')
db <- dbGetQuery(mydb, 'SELECT * FROM "Projects"')
db <- dbGetQuery(mydb, 'SELECT * FROM ProjectsProjects')
library(DBI)
db <- dbGetQuery(mydb, 'SELECT * FROM Projects')
?sqldf
install.packages("sqldf")
library(sqldf)
db <- dbGetQuery(mydb, 'SELECT * FROM file')
setwd("~/Documents/Academic/UIUC MBA/Spring 2018/Independent Study/Machine_Learning_Kickstarter")
read.csv.sql("~/Data/mlks_sample.db", sql = "select * from file")
dbDisconnect(mydb)
unlink("mlks_sample.db")
